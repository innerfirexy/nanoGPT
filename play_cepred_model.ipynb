{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from model import GPTConfig, GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data\n",
    "dataset = 'openwebtext'\n",
    "gradient_accumulation_steps = 5 * 8 # used to simulate larger batch sizes\n",
    "batch_size = 12 # if gradient_accumulation_steps > 1, this is the micro-batch size\n",
    "block_size = 1024\n",
    "\n",
    "device = 'mps'\n",
    "device_type = 'cuda' if 'cuda' in device else 'cpu' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# poor man's data loader\n",
    "data_dir = os.path.join('data', dataset)\n",
    "def get_batch(split):\n",
    "    # We recreate np.memmap every batch to avoid a memory leak, as per\n",
    "    # https://stackoverflow.com/questions/45132940/numpy-memmap-memory-usage-want-to-iterate-once/61472122#61472122\n",
    "    if split == 'train':\n",
    "        data = np.memmap(os.path.join(data_dir, 'train.bin'), dtype=np.uint16, mode='r')\n",
    "    else:\n",
    "        data = np.memmap(os.path.join(data_dir, 'val.bin'), dtype=np.uint16, mode='r')\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([torch.from_numpy((data[i:i+block_size]).astype(np.int64)) for i in ix])\n",
    "    y = torch.stack([torch.from_numpy((data[i+1:i+1+block_size]).astype(np.int64)) for i in ix])\n",
    "    if device_type == 'cuda':\n",
    "        # pin arrays x,y, which allows us to move them to GPU asynchronously (non_blocking=True)\n",
    "        x, y = x.pin_memory().to(device, non_blocking=True), y.pin_memory().to(device, non_blocking=True)\n",
    "    else:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = get_batch('val')\n",
    "print('x.shape =', x.shape, 'y.shape =', y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "out_dir = 'out-gpt2-cepred'\n",
    "ckpt_path = os.path.join(out_dir, 'ckpt.pt')\n",
    "checkpoint = torch.load(ckpt_path, map_location=device)\n",
    "\n",
    "gptconf = GPTConfig(**checkpoint['model_args'])\n",
    "model = GPT(gptconf)\n",
    "state_dict = checkpoint['model']\n",
    "\n",
    "unwanted_prefix = '_orig_mod.'\n",
    "for k,v in list(state_dict.items()):\n",
    "    if k.startswith(unwanted_prefix):\n",
    "        state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
    "model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(checkpoint))\n",
    "print(len(checkpoint))\n",
    "print(checkpoint.keys())\n",
    "print('model_args =', checkpoint['model_args'])\n",
    "\n",
    "print()\n",
    "print('type(checkpoint[\\'model\\']) = ', type(checkpoint['model']))\n",
    "print('len of checkpoint[\\'model\\'] =', len(checkpoint['model']))\n",
    "print('checkpoint[\\'model\\'].keys() =', checkpoint['model'].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the naming of a hf gpt-2 \n",
    "from transformers import AutoModelForCausalLM\n",
    "gpt2_hf = AutoModelForCausalLM.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(gpt2_hf))\n",
    "gpt2_hf_state_dict = gpt2_hf.state_dict()\n",
    "\n",
    "print('len of gpt2_hf_state_dict =', len(gpt2_hf_state_dict))\n",
    "print('gpt2_hf_state_dict.keys() =', gpt2_hf_state_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set diff between checkpoint and gpt2_hf\n",
    "diff = set(checkpoint['model'].keys()) - set(gpt2_hf_state_dict.keys())\n",
    "print(diff) # bingo, we have the ce_predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load params from checkpoint to gpt2_hf\n",
    "# print(gpt2_hf.load_state_dict)\n",
    "\n",
    "sd_keys_hf = gpt2_hf.state_dict()\n",
    "transposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']\n",
    "sd_keys_local = checkpoint['model']\n",
    "\n",
    "for k,v in checkpoint['model'].items():\n",
    "    if k in sd_keys_hf:\n",
    "        if any(k.endswith(w) for w in transposed):\n",
    "            assert sd_keys_hf[k].shape[::-1] == v.shape\n",
    "            with torch.no_grad():\n",
    "                sd_keys_hf[k].copy_(v.t())\n",
    "        else:\n",
    "            try:\n",
    "                with torch.no_grad():\n",
    "                    sd_keys_hf[k].copy_(v)\n",
    "            except RuntimeError:\n",
    "                print('RuntimeError: ', k)\n",
    "                print('sd_keys_hf[k].shape =', sd_keys_hf[k].shape)\n",
    "                print('v.shape =', v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://huggingface.co\n",
      "/Users/xy/.cache/huggingface\n",
      "innferfirexy\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "print(os.getenv('HF_ENDPOINT'))\n",
    "print(os.getenv('HF_HOME'))\n",
    "print(os.getenv('HF_USERNAME'))\n",
    "# print(os.getenv('HF_TOKEN'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30ec164127b0465280c2efc92ea7826f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Push the updated gpt2_hf to hf hub\n",
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d09198c96d314bdbb42a8723c4f5abdf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/24.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aca9550b1d494fb1bcde25f88bf29a68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/498M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/innerfirexy/gpt2-cepred/commit/b0b951f2e8bc2f669b9fc97628b120f8978e3d0e', commit_message='Upload model', commit_description='', oid='b0b951f2e8bc2f669b9fc97628b120f8978e3d0e', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "gpt2_hf = AutoModelForCausalLM.from_pretrained('models/gpt2-cepred')\n",
    "# gpt2_hf.save_pretrained('models/gpt2-cepred')\n",
    "gpt2_hf.push_to_hub('innerfirexy/gpt2-cepred')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "model.to(device)\n",
    "compile = False\n",
    "if compile:\n",
    "    model = torch.compile(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.ce_predictor)\n",
    "print(model.ce_predictor[0].weight.shape)\n",
    "print(model.ce_predictor[0].weight.data.cpu().numpy().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.lm_head.weight.shape)\n",
    "print(model.lm_head.weight.data.cpu().numpy().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear = torch.nn.Linear(768,1)\n",
    "print(linear.weight.data.cpu().numpy().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do some inference\n",
    "with torch.no_grad():\n",
    "    logits, ce_pred, loss = model(x)\n",
    "\n",
    "print('logits.shape =', logits.shape)\n",
    "print('ce_pred.shape =', ce_pred.shape)\n",
    "print('loss =', loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(ce_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    logits, ce_pred, loss = model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(model, idx):\n",
    "    device = idx.device\n",
    "    b, t = idx.size()\n",
    "    assert t <= model.config.block_size, f\"Cannot forward sequence of length {t}, block size is only {model.config.block_size}\"\n",
    "    pos = torch.arange(0, t, dtype=torch.long, device=device) # shape (t)\n",
    "\n",
    "    # forward the GPT model itself\n",
    "    tok_emb = model.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)\n",
    "    pos_emb = model.transformer.wpe(pos) # position embeddings of shape (t, n_embd)\n",
    "    x = model.transformer.drop(tok_emb + pos_emb)\n",
    "    for block in model.transformer.h:\n",
    "        x = block(x)\n",
    "    x = model.transformer.ln_f(x)\n",
    "\n",
    "    logits = model.lm_head(x)\n",
    "    ce_pred = model.ce_predictor(x)\n",
    "\n",
    "    return logits, ce_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test forward()\n",
    "logits, ce_pred = forward(model, x)\n",
    "\n",
    "print('logits.shape =', logits.shape)\n",
    "print('ce_pred.shape =', ce_pred.shape)\n",
    "\n",
    "print(ce_pred[:,:5,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get some ce_pred data and plot\n",
    "eval_iters = 100\n",
    "ce_pred_list = []\n",
    "with torch.no_grad():\n",
    "    for i in tqdm(range(eval_iters)):\n",
    "        x, _ = get_batch('val')\n",
    "        _, ce_pred = forward(model, x)\n",
    "        ce_pred_list.append(ce_pred.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save ce_pred_data\n",
    "ce_pred_data = np.concatenate(ce_pred_list, axis=0)\n",
    "print(ce_pred_data.shape)\n",
    "# remove last dimension\n",
    "ce_pred_data = ce_pred_data[:,:,0]\n",
    "print(ce_pred_data.shape)\n",
    "\n",
    "np.save('ce_pred_data.npy', ce_pred_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot\n",
    "import rpy2 \n",
    "\n",
    "%load_ext rpy2.ipython\n",
    "%R require(\"ggplot2\")\n",
    "%R require(\"data.table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ce_pred_data = np.load('ce_pred_data.npy')\n",
    "\n",
    "df_pred = pd.DataFrame(ce_pred_data)\n",
    "# df_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R -i df_pred\n",
    "\n",
    "dt <- data.table(df_pred)\n",
    "dt$sid = 1:nrow(dt)\n",
    "dt.melt <- melt(dt, id.vars = \"sid\", variable.name = \"pos\", value.name = \"ce_pred\")\n",
    "# dt.melt\n",
    "nrow(dt.melt)\n",
    "\n",
    "dt.stat <- dt.melt[, .(mean=mean(ce_pred), upper=mean(ce_pred)+sd(ce_pred), lower=mean(ce_pred)-sd(ce_pred)), by = pos]\n",
    "dt.stat\n",
    "\n",
    "# slow\n",
    "# dt.stat$pos = as.numeric(dt.stat$pos)\n",
    "# m <- lm(mean ~ pos, data = dt.stat)\n",
    "# summary(m)\n",
    "\n",
    "dt.stat$pos <- as.numeric(dt.stat$pos)\n",
    "p <- ggplot(dt.stat, aes(x = pos)) + \n",
    "    geom_ribbon(aes(ymin = lower, ymax = upper), alpha = 0.5, fill = \"grey70\") + \n",
    "    geom_line(aes(y = mean), group=1) + \n",
    "    labs(x = \"Position\", y = \"CE Pred\")\n",
    "ggsave(\"cepred_vs_pos_meanerror.pdf\", p, width=5, height=5)\n",
    "\n",
    "# dt.melt$pos <- as.numeric(dt.melt$pos)\n",
    "# p1 <- ggplot(dt.melt, aes(x = pos, y = ce_pred)) +\n",
    "#     geom_smooth(method = \"gam\") + \n",
    "#     labs(x = \"Position\", y = \"CE Pred\")\n",
    "# ggsave(\"cepred_vs_pos_smooth.pdf\", p1, width=5, height=5)\n",
    "\n",
    "p2 <- ggplot(dt.stat, aes(x = pos, y = mean)) +\n",
    "    geom_point()  + \n",
    "    labs(x = \"Position\", y = \"CE Pred\")\n",
    "ggsave(\"cepred_vs_pos_point.pdf\", p2, width=5, height=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R -i df_pred\n",
    "\n",
    "# Fit GAM model\n",
    "require(\"mgcv\")\n",
    "\n",
    "dt <- data.table(df_pred)\n",
    "dt$sid = 1:nrow(dt)\n",
    "dt.melt <- melt(dt, id.vars = \"sid\", variable.name = \"pos\", value.name = \"ce_pred\")\n",
    "\n",
    "setkey(dt.melt, sid)\n",
    "head(dt.melt)\n",
    "\n",
    "dt.melt$pos <- as.numeric(dt.melt$pos)\n",
    "gam <- gam(ce_pred ~ s(pos, bs=\"cs\"), data = dt.melt)\n",
    "\n",
    "get_GAM_pred <- function(gam_model, n_interp = 500) {\n",
    "    x <- data.table(pos = seq(0, 1000, 1))\n",
    "    y <- as.numeric(predict(gam_model, x))\n",
    "    d <- data.table(x = x$pos, y = y)\n",
    "    d\n",
    "}\n",
    "\n",
    "dt.pred <- get_GAM_pred(gam)\n",
    "dt.pred\n",
    "\n",
    "# Plot\n",
    "p <- ggplot(dt.pred, aes(x = x, y = y)) + geom_point()\n",
    "p"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
